{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements.txt\n",
    "#https://github.com/alexklibisz/elastiknn/blob/main/examples/tutorial-notebooks/multimodal-search-amazon-products.ipynb\n",
    "#https://towardsdatascience.com/computing-node-embedding-with-a-graph-database-neo4j-its-graph-data-science-library-d45db83e54b6\n",
    "\n",
    "#python -m venv .venv\n",
    "#source .venv/bin/activate\n",
    "#pip install -r requirements.txt\n",
    "#jupyter lab\n",
    "###work work work\n",
    "#deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.11.3)\n",
      "Requirement already satisfied: scikit-learn in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.23.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.26.1)\n",
      "Requirement already satisfied: nltk in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (3.5)\n",
      "Requirement already satisfied: numpy in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.13.0)\n",
      "Requirement already satisfied: scipy in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: sentencepiece in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.10.2)\n",
      "Requirement already satisfied: tqdm in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.50.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.10.2)\n",
      "Requirement already satisfied: numpy in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from torchvision->sentence-transformers) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
      "Requirement already satisfied: numpy in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: scipy in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.10.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.13.0)\n",
      "Requirement already satisfied: numpy in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: filelock in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.50.2)\n",
      "Requirement already satisfied: requests in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.4)\n",
      "Requirement already satisfied: click in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.50.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
      "Requirement already satisfied: filelock in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
      "Requirement already satisfied: requests in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: tqdm in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.50.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: numpy in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.25.11)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: packaging\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "Successfully installed packaging-23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U --use-feature=2020-resolver sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch-dsl\n",
      "  Using cached elasticsearch_dsl-7.4.1-py2.py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: six in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from elasticsearch-dsl) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from elasticsearch-dsl) (2.8.1)\n",
      "Collecting elasticsearch<8.0.0,>=7.0.0\n",
      "  Downloading elasticsearch-7.17.9-py2.py3-none-any.whl (385 kB)\n",
      "\u001b[K     |████████████████████████████████| 385 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch-dsl) (1.25.11)\n",
      "Requirement already satisfied: certifi in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch-dsl) (2020.6.20)\n",
      "Requirement already satisfied: six in /home/tiagoooliveira/anaconda3/lib/python3.8/site-packages (from elasticsearch-dsl) (1.15.0)\n",
      "Installing collected packages: elasticsearch, elasticsearch-dsl\n",
      "Successfully installed elasticsearch-7.17.9 elasticsearch-dsl-7.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --use-feature=2020-resolver elasticsearch-dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from vectordocutil import *\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint, pformat\n",
    "from IPython.display import Image, display, Markdown, Code, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "#from pymilvus import CollectionSchema, FieldSchema, DataType\n",
    "\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from faker import Faker\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import connections\n",
    "from elasticsearch_dsl.query import MultiMatch, Match\n",
    "\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "fake = Faker(['en_US'])\n",
    "fake.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(string)\n",
    "embeddingsTXT = model.encode(\" \".join(string), show_progress_bar=True)\n",
    "embeddingsTXT = np.array([embedding for embedding in embeddingsTXT]).astype(\"float32\")\n",
    "embeddings = np.array([embedding for embedding in embeddings]).astype(\"float64\")\n",
    "embeddings = np.array([embedding for embedding in embeddings]).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = [\n",
    "    [i for i in range(768)],\n",
    "    [[embeddings[j] for i in range(1)] for j in range(768)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Neo4j for extracting embeddings out of the graph via GDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup for OpenSearch\n",
    "#https://opensearch.org/downloads.html\n",
    "#1. Set up your Docker host environment\n",
    "#    * macOS & Windows: In Docker Preferences > Resources, set RAM to at least 4 GB.\n",
    "#    * Linux: Ensure vm.max_map_count is set to at least 262144 as per the documentation.\n",
    "#2. Download docker-compose.yml into your desired directory\n",
    "#3. Run docker-compose up\n",
    "#4. Have a nice coffee while everything is downloading and starting up\n",
    "#5. Navigate to http://localhost:5601/ for OpenSearch Dashboards\n",
    "#6. Login with the default username (admin) and password (admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elasticsearch: OpenSarch, OpenDistro, Elastic\n",
    "#https://www.elastic.co/guide/en/elasticsearch/client/python-api/master/connecting.html\n",
    "#https://elasticsearch-dsl.readthedocs.io/en/latest/search_dsl.html\n",
    "#https://elasticsearch-dsl.readthedocs.io/en/latest/index.html\n",
    "#curl -XGET https://localhost:9200 -u admin:admin --insecure  \n",
    "#https://github.com/elastic/elasticsearch-py/issues/712\n",
    "#curl -XGET https://localhost:9200/_cat/indices -u admin:admin --insecure \n",
    "\n",
    "client = Elasticsearch()\n",
    "connections.create_connection(hosts=['https://localhost:9200'], timeout=20, use_ssl=False, verify_certs=False,http_auth=(\"admin:admin\"))\n",
    "#scheme=\"http\", use_ssl=False, verify_certs=False, \n",
    "\n",
    "#es = Elasticsearch([\"http://localhost:9200\"])\n",
    "#es.info\n",
    "#es.cluster.health(wait_for_status='yellow', request_timeout=1)\n",
    "\n",
    "s = Search(index=\"indices\").query(\"match\", title=\"python\")\n",
    "\n",
    "\n",
    "#es.cluster.health(wait_for_status='yellow', request_timeout=1)\n",
    "\n",
    "#response = s.execute()\n",
    "#print('Total %d hits found.' % response.hits.total)\n",
    "#for h in response:\n",
    "#    print(h.title, h.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = s.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this does work on mac terminal, not via Jupyter Notebook\n",
    "#https://elasticsearch-py.readthedocs.io/en/v8.5.2/\n",
    "#https://github.com/pgolding/elasticsearch/blob/master/index.py\n",
    "\n",
    "#this flow does work via terminal, but not in jupyter notebook\n",
    "try:\n",
    "    es = Elasticsearch(\"https://admin:admin@localhost:9200\",\n",
    "                           use_ssl = False,\n",
    "                           ca_certs=False,\n",
    "                           verify_certs=False)\n",
    "    print(\"Connection to ES Server successful\")\n",
    "    \n",
    "    for i in es.indices.get_alias(\"*\"):\n",
    "        print(i)\n",
    "except:\n",
    "    print(\"Unable to connect to server\")\n",
    "    exit(1)\n",
    "#this flow does work via terminal, but not in jupyter notebook\n",
    "\n",
    "    \n",
    "doc = {\n",
    "    'author': 'kimchy',\n",
    "    'text': 'Elasticsearch: cool. bonsai cool.',\n",
    "    'timestamp': datetime.now(),\n",
    "}\n",
    "resp = es.index(index=\"test-index\", id=1, document=doc)\n",
    "print(resp['result'])\n",
    "\n",
    "resp = es.get(index=\"test-index\", id=1)\n",
    "print(resp['_source'])\n",
    "\n",
    "es.indices.refresh(index=\"test-index\")\n",
    "\n",
    "resp = es.search(index=\"test-index\", query={\"match_all\": {}})\n",
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "for hit in resp['hits']['hits']:\n",
    "    print(\"%(timestamp)s %(author)s: %(text)s\" % hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating syntetic dataset for document key-word queries combined with vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dims = 256\n",
    "reduced = iter_vectors_reduced(fname_vectors, dims=vector_dims, samples=10000)\n",
    "\n",
    "for (asin, vec) in islice(reduced(fname_vectors), 3):\n",
    "  print(asin, len(vec), vec[:3])\n",
    "\n",
    "sample = np.array([v for (_, v) in islice(reduced(fname_vectors), 20000)])\n",
    "plt.title(\"Shape: %s, mean: %.3f\" % (sample.shape, sample.mean()))\n",
    "plt.hist(np.ravel(sample), bins=40, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 'fakeDocs'\n",
    "source_no_vecs = ['tittle', 'abstract']\n",
    "\n",
    "#function to generate yield list of items to insert into elastic\n",
    "def docs():\n",
    "  for p in tqdm(iter_products(fname_products)):\n",
    "    yield { \n",
    "      \"_op_type\": \"index\", \n",
    "      \"_index\": \"h_ocid\", \n",
    "      \"_id\": p[\"asin\"], \n",
    "      \"title\": p.get(\"title\", None), \n",
    "      \"abstract\": p.get(\"abstract\", None)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y\n",
    "\n",
    "@delayed\n",
    "def incD(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "@delayed\n",
    "def addD(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 774 µs, sys: 535 µs, total: 1.31 ms\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 305 µs, sys: 398 µs, total: 703 µs\n",
      "Wall time: 1.78 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = delayed(incD)(1)\n",
    "y = delayed(incD)(2)\n",
    "z = delayed(addD)(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.47 ms, sys: 693 µs, total: 3.17 ms\n",
      "Wall time: 2.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "z.compute()\n",
    "\n",
    "#https://examples.dask.org/delayed.html\n",
    "#https://examples.dask.org/applications/evolving-workflows.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.bmc.com/blogs/elasticsearch-commands/\n",
    "#https://opensearch.org/docs/1.0/search-plugins/knn/approximate-knn/\n",
    "#https://medium.com/@bb8s/embedding-based-retrieval-approximate-nearest-neighbor-algorithms-used-in-production-systems-b96dd4b2e9a3\n",
    "#https://github.com/UKPLab/sentence-transformers/issues/1319\n",
    "#Segmentation fault\n",
    "#pip install --upgrade --force-reinstall numpy\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from faker import Faker\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "fker = Faker(['en_US'])\n",
    "\n",
    "#https://github.com/ueg1990/faker-schema\n",
    "#to generate a json\n",
    "\n",
    "#Pipeline\n",
    "#    Load json file in a collection\n",
    "#    Function returns yield item ocid and string joined ocid’s\n",
    "#    Function returns yield item ocid and vectorized string\n",
    "#    Function returns yield item ocid and Fake doc{ +vectorized string}\n",
    "#    Iteration over last item to send each item to OpenSearch and append final doc to a collection\n",
    "#    Operation to save as json the collection\n",
    "#    \n",
    "#    pipelining with dask?\n",
    "#    https://examples.dask.org/applications/prefect-etl.html\n",
    "#       https://docs.prefect.io/getting-started/installation/\n",
    "\n",
    "#load json OCID into collection\n",
    "f = open('../subtree/subtrees1.json')\n",
    "data = json.load(f) #OCID->[OCID]\n",
    "f.close()\n",
    "\n",
    "#function to convert key-list into key-(string:joined ocid words)\n",
    "def listToJoinedWords(ls):\n",
    "    \" \".join(ls)\n",
    "\n",
    "#function to convert string: joined ocid words into vectorized embedding representation\n",
    "def stringToVector(md, string):\n",
    "    md.encode(string, show_progress_bar=False)\n",
    "    \n",
    "#function receives embedded representation and creates the document for being later sent to elastic\n",
    "def vectorEnhancedDoc(key, vector, faker, index):\n",
    "    doc = {\n",
    "            \"_op_type\": \"index\", \n",
    "            \"_index\": index, \n",
    "            \"_id\": p[\"asin\"], \n",
    "            \"id\": key,\n",
    "            \"my_vector\": vector,\n",
    "            \"doc\": faker.text()\n",
    "    }\n",
    "\n",
    "def sendToRedisSearch(doc):\n",
    "    #DO send to redis\n",
    "    print(doc)\n",
    "\n",
    "#create elastic mapper datamodel\n",
    "\n",
    "#to pipeline https://examples.dask.org/delayed.html\n",
    "    \n",
    "#routine to send docs to elastic and append key-doc to a collection\n",
    "\n",
    "#routine to create a collection for later save to disk key-doc data structure. Facilitate the test\n",
    "#OCID->DOC{[OCID].toString().toVector} json.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delayed functions\n",
    "zs = []\n",
    "\n",
    "listToJoinedWordsD = dask.delayed(listToJoinedWords)\n",
    "stringToVectorD = dask.delayed(stringToVector)\n",
    "vectorEnhancedDocD = dask.delayed(vectorEnhancedDoc)\n",
    "sendToRedisSearchD = dask.delayed(sendToRedisSearch)\n",
    "\n",
    "for key in data:\n",
    "    x = listToJoinedWordsD(data[key])\n",
    "    y = stringToVectorD(model, x)\n",
    "    z = vectorEnhancedDocD(key,y,fker,\"h_ocid\")\n",
    "    send = sendToRedisSearchD(z)\n",
    "    zs.append(send)\n",
    "    \n",
    "zs = dask.persist(*zs)  # trigger computation in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Elasticsearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bulk insert\n",
    "bulk(es, docs(), chunk_size=2000, max_retries=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://opensearch.org/docs/1.0/search-plugins/knn/approximate-knn/\n",
    "PUT /h_ocid\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 768,\n",
    "        \"method\": {\n",
    "            \"name\": \"hnsw\",\n",
    "            \"space_type\": \"cosinesimil\",\n",
    "            \"engine\": \"nmslib\",\n",
    "            \"parameters\": {\n",
    "              \"ef_construction\": 256,\n",
    "              \"m\": 48\n",
    "            }\n",
    "        }\n",
    "      },\n",
    "    \"doc\": { \"type\": \"text\" }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = \"myindex\"\n",
    "source_no_vecs = ['vector']\n",
    "\n",
    "body = {\n",
    "  \"query\": {\n",
    "        \"knn\": {\n",
    "          \"my_vector\": {\n",
    "            \"vector\": [1, 1.5],\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "res = es.search(index=index, body=body, size=5, _source=source_no_vecs)\n",
    "\n",
    "#zsh: segmentation fault  python\n",
    "#https://github.com/UKPLab/sentence-transformers/issues/1319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"elastiknn\": True,\n",
    "    \"number_of_shards\": 1,\n",
    "    \"number_of_replicas\": 0\n",
    "  }\n",
    "}\n",
    "\n",
    "mapping = {\n",
    "  \"dynamic\": False,\n",
    "  \"properties\": {\n",
    "    \"asin\": { \"type\": \"keyword\" },\n",
    "    \"imVecElastiknn\": {\n",
    "      \"type\": \"elastiknn_dense_float_vector\",\n",
    "      \"elastiknn\": {\n",
    "        \"dims\": vector_dims,\n",
    "        \"model\": \"lsh\",\n",
    "        \"similarity\": \"angular\",\n",
    "        \"L\": 60,\n",
    "        \"k\": 3\n",
    "      }\n",
    "    },\n",
    "    \"imVecXpack\": {\n",
    "      \"type\": \"dense_vector\",\n",
    "      \"dims\": vector_dims\n",
    "    },\n",
    "    \"title\": { \"type\": \"text\" },\n",
    "    \"description\": { \"type\": \"text\" },\n",
    "    \"price\": { \"type\": \"float\" },\n",
    "    \"imUrl\": { \"type\": \"text\" }\n",
    "  }\n",
    "}\n",
    "\n",
    "if not es.indices.exists(index):\n",
    "  es.indices.create(index, settings)\n",
    "  es.indices.put_mapping(mapping, index)\n",
    "es.indices.get_mapping(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenSearch queries\n",
    "PUT /myindex\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 2\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "PUT /h_ocid\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 768\n",
    "      },\n",
    "    \"seq_ocid\": { \"type\": \"text\" }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "PUT /myindex/_doc/1\n",
    "{\n",
    "  \"my_vector\": [1.5, 2.5]\n",
    "}\n",
    "\n",
    "PUT /myindex/_doc/2\n",
    "{\n",
    "  \"my_vector\": [2.5, 3.5]\n",
    "} \n",
    "\n",
    "POST /myindex/_search\n",
    "{\n",
    "  \"size\": 2,\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"my_vector\": {\n",
    "        \"vector\": [1, 1.5],\n",
    "        \"k\": 5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#combine the knn query clause with other query clauses\n",
    "POST /myindex/_search\n",
    "{\n",
    "  \"size\": 5,\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": {\n",
    "        \"knn\": {\n",
    "          \"my_vector\": {\n",
    "            \"vector\": [3, 4],\n",
    "            \"k\": 5\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"range\": {\n",
    "          \"price\": {\n",
    "            \"lt\": 15\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
