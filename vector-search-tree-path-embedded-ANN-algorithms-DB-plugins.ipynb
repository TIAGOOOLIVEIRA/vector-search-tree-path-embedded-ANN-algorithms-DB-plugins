{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements.txt\n",
    "#https://github.com/alexklibisz/elastiknn/blob/main/examples/tutorial-notebooks/multimodal-search-amazon-products.ipynb\n",
    "#https://towardsdatascience.com/computing-node-embedding-with-a-graph-database-neo4j-its-graph-data-science-library-d45db83e54b6\n",
    "\n",
    "#python -m venv .venv\n",
    "#source .venv/bin/activate\n",
    "#pip install -r requirements.txt\n",
    "#jupyter lab\n",
    "###work work work\n",
    "###deactivate\n",
    "\n",
    "#docker-compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U --use-feature=2020-resolver sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --use-feature=2020-resolver elasticsearch-dsl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-gpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from vectordocutil import *\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint, pformat\n",
    "from IPython.display import Image, display, Markdown, Code, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "#from pymilvus import CollectionSchema, FieldSchema, DataType\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from faker import Faker\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import connections\n",
    "from elasticsearch_dsl.query import MultiMatch, Match\n",
    "\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "from dask import delayed\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "fake = Faker(['en_US'])\n",
    "fake.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(string)\n",
    "embeddingsTXT = model.encode(\" \".join(string), show_progress_bar=True)\n",
    "embeddingsTXT = np.array([embedding for embedding in embeddingsTXT]).astype(\"float32\")\n",
    "embeddings = np.array([embedding for embedding in embeddings]).astype(\"float64\")\n",
    "embeddings = np.array([embedding for embedding in embeddings]).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = [\n",
    "    [i for i in range(768)],\n",
    "    [[embeddings[j] for i in range(1)] for j in range(768)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Neo4j for extracting embeddings out of the graph via GDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup for OpenSearch\n",
    "#https://opensearch.org/downloads.html\n",
    "#1. Set up your Docker host environment\n",
    "#    * macOS & Windows: In Docker Preferences > Resources, set RAM to at least 4 GB.\n",
    "#    * Linux: Ensure vm.max_map_count is set to at least 262144 as per the documentation.\n",
    "#2. Download docker-compose.yml into your desired directory\n",
    "#3. Run docker-compose up\n",
    "#4. Have a nice coffee while everything is downloading and starting up\n",
    "#5. Navigate to http://localhost:5601/ for OpenSearch Dashboards\n",
    "#6. Login with the default username (admin) and password (admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elasticsearch: OpenSarch, OpenDistro, Elastic\n",
    "#https://www.elastic.co/guide/en/elasticsearch/client/python-api/master/connecting.html\n",
    "#https://elasticsearch-dsl.readthedocs.io/en/latest/search_dsl.html\n",
    "#https://elasticsearch-dsl.readthedocs.io/en/latest/index.html\n",
    "#curl -XGET https://localhost:9200 -u admin:admin --insecure  \n",
    "#https://github.com/elastic/elasticsearch-py/issues/712\n",
    "#curl -XGET https://localhost:9200/_cat/indices -u admin:admin --insecure \n",
    "\n",
    "client = Elasticsearch()\n",
    "connections.create_connection(hosts=['https://localhost:9200'], timeout=20, use_ssl=False, verify_certs=False,http_auth=(\"admin:admin\"))\n",
    "#scheme=\"http\", use_ssl=False, verify_certs=False, \n",
    "\n",
    "#es = Elasticsearch([\"http://localhost:9200\"])\n",
    "#es.info\n",
    "#es.cluster.health(wait_for_status='yellow', request_timeout=1)\n",
    "\n",
    "s = Search(index=\"indices\").query(\"match\", title=\"python\")\n",
    "\n",
    "\n",
    "#es.cluster.health(wait_for_status='yellow', request_timeout=1)\n",
    "\n",
    "#response = s.execute()\n",
    "#print('Total %d hits found.' % response.hits.total)\n",
    "#for h in response:\n",
    "#    print(h.title, h.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = s.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this does work on mac terminal, not via Jupyter Notebook\n",
    "#https://elasticsearch-py.readthedocs.io/en/v8.5.2/\n",
    "#https://github.com/pgolding/elasticsearch/blob/master/index.py\n",
    "\n",
    "#this flow does work via terminal, but not in jupyter notebook\n",
    "try:\n",
    "    es = Elasticsearch(\"https://admin:admin@localhost:9200\",\n",
    "                           use_ssl = False,\n",
    "                           ca_certs=False,\n",
    "                           verify_certs=False)\n",
    "    print(\"Connection to ES Server successful\")\n",
    "    \n",
    "    for i in es.indices.get_alias(\"*\"):\n",
    "        print(i)\n",
    "except:\n",
    "    print(\"Unable to connect to server\")\n",
    "    exit(1)\n",
    "#this flow does work via terminal, but not in jupyter notebook\n",
    "\n",
    "    \n",
    "doc = {\n",
    "    'author': 'kimchy',\n",
    "    'text': 'Elasticsearch: cool. bonsai cool.',\n",
    "    'timestamp': datetime.now(),\n",
    "}\n",
    "resp = es.index(index=\"test-index\", id=1, document=doc)\n",
    "print(resp['result'])\n",
    "\n",
    "resp = es.get(index=\"test-index\", id=1)\n",
    "print(resp['_source'])\n",
    "\n",
    "es.indices.refresh(index=\"test-index\")\n",
    "\n",
    "resp = es.search(index=\"test-index\", query={\"match_all\": {}})\n",
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "for hit in resp['hits']['hits']:\n",
    "    print(\"%(timestamp)s %(author)s: %(text)s\" % hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating syntetic dataset for document key-word queries combined with vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dims = 256\n",
    "reduced = iter_vectors_reduced(fname_vectors, dims=vector_dims, samples=10000)\n",
    "\n",
    "for (asin, vec) in islice(reduced(fname_vectors), 3):\n",
    "  print(asin, len(vec), vec[:3])\n",
    "\n",
    "sample = np.array([v for (_, v) in islice(reduced(fname_vectors), 20000)])\n",
    "plt.title(\"Shape: %s, mean: %.3f\" % (sample.shape, sample.mean()))\n",
    "plt.hist(np.ravel(sample), bins=40, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 'fakeDocs'\n",
    "source_no_vecs = ['tittle', 'abstract']\n",
    "\n",
    "#function to generate yield list of items to insert into elastic\n",
    "def docs():\n",
    "  for p in tqdm(iter_products(fname_products)):\n",
    "    yield { \n",
    "      \"_op_type\": \"index\", \n",
    "      \"_index\": \"h_ocid\", \n",
    "      \"_id\": p[\"asin\"], \n",
    "      \"title\": p.get(\"title\", None), \n",
    "      \"abstract\": p.get(\"abstract\", None)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y\n",
    "\n",
    "@delayed\n",
    "def incD(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "@delayed\n",
    "def addD(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = delayed(incD)(1)\n",
    "y = delayed(incD)(2)\n",
    "z = delayed(addD)(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()\n",
    "\n",
    "#https://examples.dask.org/delayed.html\n",
    "#https://examples.dask.org/applications/evolving-workflows.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "#https://www.bmc.com/blogs/elasticsearch-commands/\n",
    "#https://opensearch.org/docs/1.0/search-plugins/knn/approximate-knn/\n",
    "#https://medium.com/@bb8s/embedding-based-retrieval-approximate-nearest-neighbor-algorithms-used-in-production-systems-b96dd4b2e9a3\n",
    "#https://github.com/UKPLab/sentence-transformers/issues/1319\n",
    "\n",
    "#https://github.com/ueg1990/faker-schema\n",
    "#to generate a json\n",
    "\n",
    "#Pipeline\n",
    "#    Load json file in a collection\n",
    "#    Function returns yield item ocid and string joined ocidâ€™s\n",
    "#    Function returns yield item ocid and vectorized string\n",
    "#    Function returns yield item ocid and Fake doc{ +vectorized string}\n",
    "#    Iteration over last item to send each item to OpenSearch and append final doc to a collection\n",
    "#    Operation to save as json the collection\n",
    "#    \n",
    "#    pipelining with dask?\n",
    "#    https://examples.dask.org/applications/prefect-etl.html\n",
    "#       https://docs.prefect.io/getting-started/installation/\n",
    "\n",
    "#load json OCID into collection\n",
    "f = open('data/data_tree.json')\n",
    "data = json.load(f) #OCID->[OCID]\n",
    "f.close()\n",
    "\n",
    "#function to convert key-list into key-(string:joined ocid words)\n",
    "def listToJoinedWords(ls):\n",
    "    r = \" \".join([str(x) for x in ls])\n",
    "    print(r)\n",
    "    return r\n",
    "\n",
    "#function to convert string: joined ocid words into vectorized embedding representation\n",
    "def stringToVector(md, txt):\n",
    "    #ret = md.encode(txt, show_progress_bar=False)\n",
    "    #print(ret)\n",
    "    ret = np.random.random(768)\n",
    "    return ret\n",
    "    \n",
    "#function receives embedded representation and creates the document for being later sent to elastic\n",
    "def vectorEnhancedDoc(key, vector, faker, index):\n",
    "    doc = {\n",
    "            \"_op_type\": \"index\", \n",
    "            \"_index\": index, \n",
    "            \"_id\": key,\n",
    "            \"my_vector\": vector,\n",
    "            \"doc\": faker.text()\n",
    "    }\n",
    "    print(doc)\n",
    "    return doc\n",
    "\n",
    "def sendToRedisSearch(doc):\n",
    "    print(doc)\n",
    "\n",
    "#create elastic mapper datamodel\n",
    "\n",
    "#to pipeline https://examples.dask.org/delayed.html\n",
    "    \n",
    "#routine to send docs to elastic and append key-doc to a collection\n",
    "\n",
    "#routine to create a collection for later save to disk key-doc data structure. Facilitate the test\n",
    "#OCID->DOC{[OCID].toString().toVector} json.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#241000120264\n",
    "#190011801549\n",
    "#data[\"190056234585\"]\n",
    "#listToJoinedWords(data[\"190056234585\"])\n",
    "x = np.random.random(768)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating GPU acceleration for enconder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist as scipy_cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to sentence_transformer - Sentence Embeddings/Enconders/Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/lett-digital/nlp-efficient-semantic-similarity-search-with-faiss-facebook-ai-similarity-search-and-gpus-274771d0709a\n",
    "#https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/\n",
    "#https://huggingface.co/blog/how-to-train-sentence-transformers\n",
    "#traing my own sentence-transformer (to capture relationship with childs in the branch-tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "#delayed functions\n",
    "zs = []\n",
    "\n",
    "listToJoinedWordsD = dask.delayed(listToJoinedWords)\n",
    "stringToVectorD = dask.delayed(stringToVector)\n",
    "vectorEnhancedDocD = dask.delayed(vectorEnhancedDoc)\n",
    "sendToRedisSearchD = dask.delayed(sendToRedisSearch)\n",
    "\n",
    "for key in data:\n",
    "    x = listToJoinedWordsD(data[key])\n",
    "    y = stringToVectorD(model, x)\n",
    "    z = vectorEnhancedDocD(key,y,fake,\"h_ocid\")\n",
    "    send = sendToRedisSearchD(z)\n",
    "    zs.append(send)\n",
    "    \n",
    "zs = dask.persist(*zs)  # trigger computation in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Elasticsearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bulk insert\n",
    "bulk(es, docs(), chunk_size=2000, max_retries=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://opensearch.org/docs/1.0/search-plugins/knn/approximate-knn/\n",
    "PUT /h_ocid\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 768,\n",
    "        \"method\": {\n",
    "            \"name\": \"hnsw\",\n",
    "            \"space_type\": \"cosinesimil\",\n",
    "            \"engine\": \"nmslib\",\n",
    "            \"parameters\": {\n",
    "              \"ef_construction\": 256,\n",
    "              \"m\": 48\n",
    "            }\n",
    "        }\n",
    "      },\n",
    "    \"doc\": { \"type\": \"text\" }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = \"myindex\"\n",
    "source_no_vecs = ['vector']\n",
    "\n",
    "body = {\n",
    "  \"query\": {\n",
    "        \"knn\": {\n",
    "          \"my_vector\": {\n",
    "            \"vector\": [1, 1.5],\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "res = es.search(index=index, body=body, size=5, _source=source_no_vecs)\n",
    "\n",
    "#zsh: segmentation fault  python\n",
    "#https://github.com/UKPLab/sentence-transformers/issues/1319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"elastiknn\": True,\n",
    "    \"number_of_shards\": 1,\n",
    "    \"number_of_replicas\": 0\n",
    "  }\n",
    "}\n",
    "\n",
    "mapping = {\n",
    "  \"dynamic\": False,\n",
    "  \"properties\": {\n",
    "    \"asin\": { \"type\": \"keyword\" },\n",
    "    \"imVecElastiknn\": {\n",
    "      \"type\": \"elastiknn_dense_float_vector\",\n",
    "      \"elastiknn\": {\n",
    "        \"dims\": vector_dims,\n",
    "        \"model\": \"lsh\",\n",
    "        \"similarity\": \"angular\",\n",
    "        \"L\": 60,\n",
    "        \"k\": 3\n",
    "      }\n",
    "    },\n",
    "    \"imVecXpack\": {\n",
    "      \"type\": \"dense_vector\",\n",
    "      \"dims\": vector_dims\n",
    "    },\n",
    "    \"title\": { \"type\": \"text\" },\n",
    "    \"description\": { \"type\": \"text\" },\n",
    "    \"price\": { \"type\": \"float\" },\n",
    "    \"imUrl\": { \"type\": \"text\" }\n",
    "  }\n",
    "}\n",
    "\n",
    "if not es.indices.exists(index):\n",
    "  es.indices.create(index, settings)\n",
    "  es.indices.put_mapping(mapping, index)\n",
    "es.indices.get_mapping(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenSearch queries\n",
    "PUT /myindex\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 2\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "PUT /h_ocid\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 768\n",
    "      },\n",
    "    \"seq_ocid\": { \"type\": \"text\" }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "PUT /myindex/_doc/1\n",
    "{\n",
    "  \"my_vector\": [1.5, 2.5]\n",
    "}\n",
    "\n",
    "PUT /myindex/_doc/2\n",
    "{\n",
    "  \"my_vector\": [2.5, 3.5]\n",
    "} \n",
    "\n",
    "POST /myindex/_search\n",
    "{\n",
    "  \"size\": 2,\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"my_vector\": {\n",
    "        \"vector\": [1, 1.5],\n",
    "        \"k\": 5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#combine the knn query clause with other query clauses\n",
    "POST /myindex/_search\n",
    "{\n",
    "  \"size\": 5,\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": {\n",
    "        \"knn\": {\n",
    "          \"my_vector\": {\n",
    "            \"vector\": [3, 4],\n",
    "            \"k\": 5\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"range\": {\n",
    "          \"price\": {\n",
    "            \"lt\": 15\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
